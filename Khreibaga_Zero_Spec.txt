
===== PAGE 1 =====
Khreibaga Zero: Technical Specification
A Reference Manual for AlphaZero-Style Reinforcement Learning
System Architecture & Implementation Guide
February 6, 2026
Abstract
This document serves as the authoritative reference for developing an AlphaZero-based Re-
inforcement Learning agent for the game ofKhreibaga(Mauritanian Alquerque). It provides
a rigorous definition of the game environment, including edge cases for “Flying Kings” and “Ma-
jority Capture.” Furthermore, it details the state-space representation, the source-target action
space (N= 625), and the specific modifications required for the Monte Carlo Tree Search
(MCTS) algorithm to handle multi-step compulsory captures.
Contents
1 Part I: The Environment Specification (Game Logic) 3
1.1 1.1 Board Geometry & Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 1.2 Piece Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2.1 Men (Unpromoted) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2.2 Kings (Promoted) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 1.3 Capturing Logic (The Critical Path) . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 1.4 Terminal Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Part II: Reinforcement Learning Specification 6
2.1 2.1 State Representation (Input Tensor) . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 2.2 Action Space (Output Vector) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.1 Interpretation of Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 2.3 Handling Multi-Step Actions (Chains) . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Part III: Implementation Algorithms 8
3.1 3.1 The Legal Move Generator (With Majority Rule) . . . . . . . . . . . . . . . . . . 8
3.2 3.2 Neural Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1

===== PAGE 2 =====
3.3 3.3 The Training Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 Part IV: Specific Implementation Difficulties 9
4.1 4.1 Flying King Branching Factor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 4.2 Cyclic States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 4.3 First-Move Advantage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2

===== PAGE 3 =====
1 Part I: The Environment Specification (Game Logic)
TheaccuracyoftheRLagentdependsentirelyonthefidelityoftheenvironment. The“RuleEngine”
must strictly enforce every constraint defined below.
1.1 1.1 Board Geometry & Initialization
•Topology:A5×5grid of intersecting lines.
•Topology:A5×5grid of points forming 16 internal squares.
•Connectivity Constraints:
–Orthogonal connections exist between all adjacent horizontal and vertical points.
– Diagonal connections are alternating.
–A point(r, c)has diagonal connections if and only if(r+c)is even (assuming a checker-
board parity of the squares themselves, or hard-coded based on the visual pattern).
–Visual Rule:Only squares that correspond to "dark squares" on a chessboard (or vice
versa depending on indexing) contain ’X’ crosses. Squares between them are empty of
diagonals.
•Coordinate System:We utilize a 0-indexed integer system for efficiency.
–Rows:0(Bottom) to4(Top).
–Columns:0(Left) to4(Right).
– Flat Indexing:SquareID=Row×5 +Col. (Range0. . .24).
•Initial Setup:
– Total Points:25.
– Piece Count:12 Black (Player 1), 12 White (Player 2).
– Configuration:All points filledexceptthe center point (Index 12 / C3).
– Player 1 (Black):Occupies Rows 0 and 1 fully, plus indices 10 and 11 (left side of Row
2).
– Player 2 (White):Occupies Rows 3 and 4 fully, plus indices 13 and 14 (right side of
Row 2).
1.2 1.2 Piece Mechanics
1.2.1 Men (Unpromoted)
•Simple Move:One step to an adjacent empty point.
•Directionality:StrictlyForwardorForward-Diagonalrelativetotheplayer’sside. (Black
movesRow+, White movesRow−).
•Backward Movement:Illegal for simple moves. Permittedonlyduring capture sequences.
3

===== PAGE 4 =====
•Promotion:A Man is immediately promoted to aKingif it ends its turn on the opponent’s
back rank (Row 4 for Black, Row 0 for White).
–Note:If a Man reaches the promotion rankduringa multi-capture sequence but must
continue jumping out of the rank, it doesnotpromote. Promotion only occurs when the
piecestopson the final rank.
1.2.2 Kings (Promoted)
•Type:“Flying Kings” (similar to International Draughts).
•Movement:Can move any distance along a clear line (orthogonal or diagonal) to an empty
square.
•Obstructions:Cannot jump over friendly pieces. Cannot move through enemy pieces with-
out capturing.
1.3 1.3 Capturing Logic (The Critical Path)
In Khreibaga, capturing iscompulsoryand governed by strict priority rules.
1.Compulsory Capture:If any legal capture exists on the board, non-capturing moves are
illegal.
2.Man Capture:Leaps over an adjacent enemy to the empty square immediately behind. Can
capture inany direction(360 degrees).
3.King Capture (Long Leap):Leaps over a single enemy piece along a line. The King may
land onanyempty square beyond the captured piece, provided the path is clear.
4.Immediate Removal:The captured piece is removed from the boardinstantly.
•Implication:A piece may cross the same square twice in one sequence if the removal
clears the path.
5.The Majority Capture Rule:
•If multiple capture sequences are available, the playermustchoose a sequence that
captures the maximum number of pieces.
•Example:If Option A captures 2 pieces and Option B captures 3 pieces, Option A is
strictly illegal.
•Tie-Breaking:If two sequences both capture the maximum number (e.g., both 3), the
player may choose either.
1.4 1.4 Terminal Conditions
The game ends when:
4

===== PAGE 5 =====
1.Elimination:One player has 0 pieces remaining.
2.Stalemate:The current player has pieces but no legal moves (immobilized). This counts as
a loss for the immobilized player.
3.Draws:
•50-Move Rule:50 half-moves (25 turns each) pass without a capture or a Man moving.
•3-Fold Repetition:The exact same board state occurs 3 times.
5

===== PAGE 6 =====
2 Part II: Reinforcement Learning Specification
This section defines the input/output interfaces for the Neural Network.
2.1 2.1 State Representation (Input Tensor)
The board state is converted into a tensor of shape5×5×C. We recommendC= 7planes to
capture full context.
Canonical Form:Before inputting to the network, the board is always oriented such that the
current playeris moving from “bottom to top.” This allows the network to learn a single set of
features regardless of color.
Plane Feature Description
0 P1 Pieces (Men) 1 if current player’s Man is present, else 0.
1 P1 Pieces (Kings) 1 if current player’s King is present, else 0.
2 P2 Pieces (Men) 1 if opponent’s Man is present, else 0.
3 P2 Pieces (Kings) 1 if opponent’s King is present, else 0.
4 Repetitions 1 if this board configuration has appeared before
(to avoid cycles).
5 Colour Plane All 0s for White, All 1s for Black (Optional,
helps if dynamics differ).
6 Move Count Normalized scalar (t/MAX_STEPS) to sensing
draw proximity.
Table 1: Input Feature Planes
2.2 2.2 Action Space (Output Vector)
We utilize aSource-Target (From-To)discrete action space. This is the optimal representation
for “Flying Kings” where the landing square is variable.
•Total Actions:25×25 = 625logits.
•Mapping Logic:
Action Index= (SourceSquare×25) +TargetSquare
•SourceSquare:Integer0. . .24.
•TargetSquare:Integer0. . .24.
2.2.1 Interpretation of Indices
The neural network outputs a probability distribution over these 625 indices.
6

===== PAGE 7 =====
•Index 0 (0→0):Illegal (piece cannot move to itself).
•Index 6 (0→6):Move from(0,0)to(1,1).
•Index 24 (0→24):Move from(0,0)to(4,4)(Long diagonal jump).
AmbiguityResolution:TheActionSpacedoesnotdistinguishbetweena“Move” anda“Capture.”
The Environment resolves this based on the board state. If a piece atAmoves toBand jumps an
enemy, it is a capture. If the path is clear, it is a move.
2.3 2.3 Handling Multi-Step Actions (Chains)
This is the most critical implementation detail for Khreibaga.
The Atomic Action Principle
The Neural Network does not output a full sequence (e.g., A1-C3-E5). It outputs one atomic
hop at a time. The environment maintains the “Turn State.”
The Sequence Flow:
1.StateS 0:Player 1 has a multi-jump available starting at square 0.
2.Masking:The environment masks ALL moves except the valid jumps from square 0.
3.NN Action:Agent picks0→12.
4.Environment:
•Executes jump0→12.
•Removes captured piece.
•Checks: “Can piece at 12 capture again?”→YES.
•Does NOT switch player.
•Returns StateS 0.5(Intermediate state).
5.StateS 0.5:
6.Masking:Mask is zeros everywhere except valid jumps starting from square 12.
7.NN Action:Agent picks12→24.
8.Environment:
•Executes jump.
•Checks: “Can piece at 24 capture again?”→NO.
•Switches player.
•Returns StateS 1(New Turn).
7

===== PAGE 8 =====
3 Part III: Implementation Algorithms
3.1 3.1 The Legal Move Generator (With Majority Rule)
This function must be implemented in the Environment class. It is the gatekeeper for the Neural
Network.
Algorithm 1Legal Move Mask Generation
1:functionGetActionMask(BoardState)
2:Mask←Zeros(625)
3:AllMoves←[]
4:CaptureChains←[]
▷Phase 1: Detect all possible moves for current player
5:forpieceinCurrentPlayerPiecesdo
6:chains←FindMaxCaptureChains(piece, BoardState)
7:ifchainsis not emptythen
8:CaptureChains.extend(chains)
9:else
10:AllMoves.extend(FindSimpleMoves(piece))
11:end if
12:end for
▷Phase 2: Apply Majority Rule Priority
13:ifCaptureChainsis not emptythen
14:MaxLen←max(len(c)forcinCaptureChains)
15:BestChains←[cforcinCaptureChainsiflen(c) ==MaxLen]
16:forchaininBestChainsdo
17:FirstHop←chain[0]▷Extract only the immediate next atomic move
18:Index←(FirstHop.src×25) +FirstHop.dst
19:Mask[Index]←1
20:end for
21:else▷No captures available, allow simple moves
22:formoveinAllMovesdo
23:Index←(move.src×25) +move.dst
24:Mask[Index]←1
25:end for
26:end if
27:returnMask
28:end function
3.2 3.2 Neural Network Architecture
Giventhe5×5board, alargeResNet(likeinAlphaGo)isoverkill. Wedefineacompactarchitecture.
•Input:(B,7,5,5)
•Convolutional Block:Conv2d(64 filters,3×3), BN, ReLU.
8

===== PAGE 9 =====
•Residual Tower:6 Blocks.
–Each Block: Conv(64)→BN→ReLU→Conv(64)→BN→Skip Connection→ReLU.
•Policy Head:
–Conv(2 filters,1×1)→BN→ReLU.
–Flatten→Linear(50→625).
–Output: Logits (Do not apply Softmax here; Softmax is part of the loss function).
•Value Head:
–Conv(1 filter,1×1)→BN→ReLU.
–Flatten→Linear(25→64)→ReLU→Linear(64→1).
–Output: Tanh (Range -1 to 1).
3.3 3.3 The Training Loop
The self-play cycle follows the standard AlphaZero definition:
1.Self-Play:The best current model plays against itself.
•Execute 200 MCTS simulations per turn.
•Exploration:For the first 10 moves, sample actions from the MCTS visit distribution
(Temperatureτ= 1).
•Exploitation:After 10 moves, select the action with the highest visit count (τ→0).
•Store tuples(State, Policy π, Reward z).
2.Optimization:
•Sample a batch of games.
•Loss:l= (z−v)2−πTlog(p) +c∥θ∥2.
•Update weights using SGD or Adam.
3.Evaluation:
•Every 50 training steps, pit New Model vs Old Model (50 games).
•If New Model wins>55%of games, it becomes the new data generator.
4 Part IV: Specific Implementation Difficulties
4.1 4.1 Flying King Branching Factor
Problem:A King capturing a piece might have 3 different squares it can land on behind the enemy.
Solution:The “Source-Target” action space naturally handles this.A1→C3andA1→D4are
distinct indices. The Legal Move Generator must verify which landing spots are valid (i.e., path is
clear up to the jump, and destination is empty).
9

===== PAGE 10 =====
4.2 4.2 Cyclic States
Problem:Players may shuffle pieces back and forth to avoid losing.Solution:1. Include the
“Repetition Plane” in the input tensor. 2. In the Environment, store a hash of every board state in
the current game history. 3. If a move results in a hash that appeared 2 times previously, declare
the game a Draw immediately.
4.3 4.3 First-Move Advantage
Problem:On a5×5board, the first player (Black) likely has a strong theoretical advantage.
Solution:During training, strictly enforce the temperature parameterτ= 1for the opening
phase. This forces the agent to explore suboptimal openings, preventing it from overfitting to a
single “winning” line that might actually be brittle.
10
